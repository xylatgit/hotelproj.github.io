---
title: "Model Building"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
  
---
```{r setup, include=FALSE}
library(tidyverse)
library(plotly)
library(modelr)
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
fig.width = 9, 
  fig.height = 9,
  out.width = "80%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

<img src="./images/model.jpg">\
_image source: google_


```{r include=FALSE}
# data import 
hotel_df = read.csv("./data/hotel_bookings.csv",na = c("", "NA", "NULL")) 

hotel_ml = 
  hotel_df %>% 
  filter(arrival_date_year==2016) %>%
  mutate(
    arrival_season = case_when(
      arrival_date_month %in% c("October","November","December") ~ "Fall",
      arrival_date_month %in% c("January","February","March") ~ "Winter",
      arrival_date_month %in% c("April","May","June") ~ "Spring",
      TRUE ~ "Summer"
    ),
    false_room_assignment = case_when(
      reserved_room_type == assigned_room_type ~ "0",
      reserved_room_type != assigned_room_type ~ "1"
    ),
    false_room_assignment = as.factor(false_room_assignment),
    is_canceled = as.factor(is_canceled)) %>% 
    select(lead_time,total_of_special_requests,agent,previous_cancellations,adr,booking_changes,arrival_date_month,arrival_season,meal,market_segment,distribution_channel,reserved_room_type,deposit_type,customer_type,is_canceled) %>% 
  mutate_if(is.character, as.factor)

##write hotel_ml.csv
write.csv(hotel_ml,"./data/hotel_ml.csv",row.names = F)

```

## Overview
Using existing data, we investigated factors that are associated with - and can potentially be used to predict - hotel booking cancellation.\
Two types of models are used for prediction - logistic regression and random forest. \
Categorical variables are selected based on our hypothesis on its association with cancellation, and for numeric variables, we make the selected based on AUC scores.\
Overall, the logistic regression model achieves an accuracy of 75%. The accuracy for the random forest model is about 86%.

## Variable Selection
The dataset is quite large (119,390 rows, 32 variables). For a speedy performance of our models, we first selected instances in year 2016 alone (the dataset ranges from 2015 to 2017), which resulted in 56,707 rows.\
\
For categorical variables, we selected 8 out of 14 variables based on our hypothesis on their association with _IsCanceled_.\
\
For numeric variables, we selected 5 variables based on their AUC scores.
```{r AUC,echo=FALSE, message=F }
library(dplyr)
library(tidyr)
library(pROC)
library(purrr)
library(readr)

 hotel_df_xyl <- read.csv("./data/hotel_bookings.csv",na = c("", "NA", "NULL")) %>%
   select(where(is.numeric)) %>% select(-agent)

 variable_list <- colnames(hotel_df_xyl)
 variable_list <- variable_list[-1]

 compute_auc <- function(x) {
   as.numeric(auc(roc(hotel_df_xyl[["is_canceled"]][!is.na(hotel_df_xyl[[x]])], hotel_df_xyl[[x]][!is.na(hotel_df_xyl[[x]])])))
 }

 auc_result<-as_tibble(t(combn(variable_list, 1)),
           .name_repair = ~ c("var_name"))%>%
   mutate(
     auc = map_dbl(.x = var_name, compute_auc)
   ) %>%
   arrange(-auc)
 auc_result %>% knitr::kable(digits = 3)

```

An AUC score equals 0.5 implies that the variable is not discriminable between two classes. Thus, we chose variables that had the greatest distances to 0.5.\
From the AUC score table above, we chose _LeadTime_, _TotalOfSpecialRequests_, _PreviousCancellations_, _adr_, and _BookingChanges_ for our models.


## Logistic Regression  

```{r data import,echo=FALSE, message=F}
hotel_ml = 
  read_csv("./data/hotel_ml.csv") %>% 
  mutate(
    is_canceled = as.factor(is_canceled)
  ) %>% 
  mutate_if(is.character, as.factor) %>% 
  select(-arrival_date_month) %>% 
  drop_na() 
```

#### Regression Output  
```{r  include=FALSE, echo=FALSE, message=F}
fullmod = glm(is_canceled ~. , data = hotel_ml, family = binomial())
finalmod = 
  stats::step(fullmod, direction = "both") %>% 
  broom::tidy() %>% 
  knitr::kable()
```
We used stepwise selection and concluded that all of our existing variables have statistical significance. We thus fitted this model to our dataset, and generated the following output for the variables with significant odds ratios:  
```{r test/train split,echo=FALSE, message=F}
set.seed(1234)
train_df = sample_frac(hotel_ml, 0.8)
test_df = anti_join(hotel_ml, train_df)

logistic_fit =  
  train_df %>% 
  glm(is_canceled ~. , data = ., family = binomial())

logistic_fit %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  filter(OR > 1.9 | OR < 0.2) %>% 
  arrange(desc(OR)) %>% 
  knitr::kable(digits = 3, caption = "Logistic Regression Output") 
```

**Comment** As we can see from the output, deposit type: non refund has the largest odds ratio(173.389), indicating that the guests with non refundable test are significantly more likely to cancel the bookings compared to no deposit guests. This finding is rather counter-intuitive and also raises the question about the validity of the dataset. 

#### Variable Importance 
We furthur generated the most important variables of our model:  
```{r variable importance,echo=FALSE, message=F}
caret::varImp(logistic_fit) %>%
  arrange(desc(Overall)) %>%
  filter(Overall > 10) %>% 
  knitr::kable(digits = 3, caption = "Variable Importance")
```

We will look more into the four most important features:  
* Total special requests
* deposit type 
* adr  
<br> 

##### The Special Request Effect

```{r specialrequest, echo=F, message=F, warning=F}
special_request_df <- 
  hotel_df %>%
  mutate(is_canceled = recode(is_canceled, "1" = "Canceled", "0" = "Not Canceled")) %>%
  select(hotel, is_canceled, total_of_special_requests)


special_request_prop <- 
  special_request_df %>%
  group_by(total_of_special_requests, hotel) %>%
  summarize(
    guest_total = n(), 
    guest_canceled = sum(is_canceled == "Canceled")
  ) %>%
  mutate(
    prop_test = map2(.x = guest_canceled, .y = guest_total, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>%
  select(-prop_test) %>%
  unnest() %>%
  select(total_of_special_requests, hotel, estimate, conf.low, conf.high)


special_request_prop_plot <- 
  special_request_prop %>%
    plot_ly(
    x = ~total_of_special_requests, y = ~estimate, color = ~hotel,
    type = "bar", colors = "viridis"
  ) %>%
  layout(title = "Canceled Proportion with Total Number of Special Request",
         xaxis = list(title = "Number of Special Requests"),
         yaxis = list(title = "Proportion of Cancellation", range = c(0, 1)), 
         barmode = "group"
)

special_request_prop_plot
```

Based on the proportion plot, we found that, for city hotel, the cancellation proportion is the highest when the guests didn't ask for any special request. Similarly, for resort hotel, the cancellation proportion is also higher when guests didn't ask for any special request. 
 
<br> 
##### The Deposit Type Effect

```{r deposit, echo=F, message=F, warning=F}
deposit_df <- 
  hotel_df %>%
  mutate(is_canceled = recode(is_canceled, "1" = "Canceled", "0" = "Not Canceled")) %>%
  select(hotel, is_canceled, deposit_type)


deposit_prop <- 
  deposit_df %>%
  group_by(deposit_type, hotel) %>%
  summarize(
    guest_total = n(), 
    guest_canceled = sum(is_canceled == "Canceled")
  ) %>%
  mutate(
    prop_test = map2(.x = guest_canceled, .y = guest_total, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>%
  select(-prop_test) %>%
  unnest() %>%
  select(deposit_type, hotel, estimate, conf.low, conf.high)


deposit_prop_plot <- 
  deposit_prop %>%
  mutate(text_label = str_c("\nCancellation Proportion: ", estimate)) %>%
    plot_ly(
    x = ~deposit_type, y = ~estimate, color = ~hotel,
    type = "bar", colors = "viridis", text = ~text_label
  ) %>%
  layout(title = "Canceled Proportion with Deposit Type",
         xaxis = list(title = "Deposit Type"),
         yaxis = list(title = "Proportion of Cancellation", range = c(0, 1)), 
         barmode = "group"
)

deposit_prop_plot
```

Based on the proportion plot, we found that for both resort and city hotels, reservations with non-refundable deposit have about 94% and 99% of cancellation respectively, which is significantly higher than other two types of deposits. For reservations with refundable deposit, we found that the cancellation of city hotel is relatively higher than that of resort hotel. This counter intuitive finding confirmed with the previous odds ratios. We therefore suspected the validity of this variable.  

<br>
##### The ADR Effect
```{r echo=F, message=F, warning=F}
hotel_df %>% 
  select(is_canceled, adr) %>% 
  group_by(adr) %>% 
  summarize(
    total = n(),
    canceled = sum(is_canceled == 1),
    prop = round(canceled/total, 2)
  ) %>% 
  filter(total > 10) %>% 
  drop_na() %>% 
  mutate(
    fv = lm(prop ~ adr, .) %>%  fitted.values()
  ) %>% 
  plot_ly(x = ~adr, y = ~prop, mode = "markers") %>%
  add_markers(y = ~prop) %>% 
  add_trace(x = ~adr, y = ~fv, mode = "lines") %>% 
  layout(title = "Average Daily Rate (ADR) and Cancellation", 
         xaxis = list(title = "ADR"), 
         yaxis = list(title = "Cancellation Proportion"),
         showlegend = F)
```

From this plot, there is no clear trend detected. We see that the proportion of cancelation slight increases with increase in ADR.

#### Cross Validation  
Since we are questioning the validity of deposit type, we fitted the logistic regression models without the deposit type. We then compared the prediction accuracy of the two models:
```{r ,echo=FALSE, message=F}
logistic_fit_2 = 
  train_df %>% 
  glm(is_canceled ~.-deposit_type , data = ., family = binomial())

test_df %>% 
  add_predictions(logistic_fit) %>% 
  mutate(
    prob_model_1 = boot::inv.logit(pred),
    pred_mod1 = 
      case_when(
        prob_model_1 < 0.5 ~ 0,
        prob_model_1 >= 0.5 ~ 1
         )) %>% 
  select(-pred, -prob_model_1) %>% 
  add_predictions(logistic_fit_2) %>% 
   mutate(
    prob_model_2 = boot::inv.logit(pred),
    pred_mod2 = 
      case_when(
        prob_model_2 < 0.5 ~ 0,
        prob_model_2 >= 0.5 ~ 1
         )) %>% 
  select(is_canceled, pred_mod1, pred_mod2) %>% 
  summarise(
    model1_accuracy = sum(is_canceled==pred_mod1)/ nrow(test_df),
    model2_accuracy = sum(is_canceled==pred_mod2)/ nrow(test_df)
  ) %>% 
  knitr::kable(digits = 3, 
               caption = "Model Accuracy",  
               col.names = c("Model1 accuracy", "Model2 accuracy"))
```
We can see that the two models have roughly the same prediction accuracy! We also generated the ROC for comparison:  
```{r echo=FALSE, message=F}
library(ROCR)

test.predicted.m1 <- predict(logistic_fit, newdata = test_df, type = "response")

test.predicted.m2 <- predict(logistic_fit_2, newdata = test_df, type = "response")

par(mfrow=c(1, 2))

prediction(test.predicted.m1, test_df$is_canceled) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

prediction(test.predicted.m2, test_df$is_canceled) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()
```


#### Model Diagnosis 
```{r}
train_df =
  broom::augment(logistic_fit_2) %>% 
  mutate(index = 1:n()) 

ggplot(data = train_df, aes(index, .std.resid, color = "viridis")) + 
  geom_point(alpha = .5) +
  geom_ref_line(h = 3) + 
  labs(title = "Residual Deviance plot")

plot(logistic_fit_2, which = 4, id.n = 5)
```
   
We created the residual deviance plot to detect outliers. And as shown in the plot, those standardized residuals that exceed 3 represent possible outliers and may deserve closer attention. Similarly, plotting the cook's distance also signals some potential outliers in our dataset.  


## Random Forest

```{r rf_setup, echo = F, message=F}
library(reshape2)
library(randomForest)
library(dplyr)
library(ggplot2)
library(caret)

hotel_ml_rf <- read.csv("./data/hotel_ml.csv") %>% select(-agent)
```

#### Check NAs
```{r rf_NA, echo = F, message=F}
hotel_ml_rf %>% is.na %>% melt %>% 
  ggplot(data = .,aes(x = Var2, y = Var1)) + geom_tile(aes(fill = value,width=0.95)) +
  scale_fill_manual(values = c("lightblue","white")) + theme_minimal(14) + 
  theme(axis.text.x  = element_text(angle=45, vjust=.75), 
        legend.position='None',
        legend.direction='horizontal',
        panel.grid.major=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        plot.margin=unit(c(.1,.1,.1,.1), "cm")) + 
  labs(title = 'Missing values in each column', subtitle='represented as white bars')
```
From the plot we do not any NAs in the dataset.

#### Random Forest Model
```{r randomForest, echo = F, message = F}
#Data Partition
set.seed(123)
ind <- sample(2, nrow(hotel_ml_rf), replace = T, prob = c(0.8, 0.2))
hotel_ml_rf$is_canceled <- as.factor(hotel_ml_rf$is_canceled)
train <- hotel_ml_rf[ind==1,]
test <- hotel_ml_rf[ind==2,]

#Random Forest
set.seed(222)
rf <- randomForest(is_canceled ~ ., data = train, importance = T)
print(rf)
#attributes(rf)

#confusion matrix
rf$confusion %>% knitr::kable(digits = 3)


#Predication & Confusion Matrix - train data
#p1 <- predict(rf, train)
#confusionMatrix(p1,train$is_canceled)


#Predication & Confusion Matrix - test data
p2 <- predict(rf, test)
confusionMatrix(p2,test$is_canceled)
```
Train:Test = 8:2 \ 
The error rate for the model is 14.98%. The accuracy of the model on test set is 84.93%.

#### Error Rate
```{r rf error rate, echo = F, message = F}

#Error rate of Random Forest
plot(rf, main = "Error Rate")

```
                         
From the plot on error rate for random forest model, we can see that the error rate does not change much after 100.

#### Tune mtry
```{r tune mtry, echo = F, message = F}
#Tune mtry
t <-tuneRF(train[,-14], train[,14],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 200,
       trace = TRUE,
       improve = 0.05)
t

```
Mtry refers to the number of variables available for splitting at each tree node.\
For tuning Mtry, we set paramter _ntreeTry_ (number of trees used at tuning step) to 200, since from the error rate plot we can see that ER does not change much after 200.\
From the plot we can see that the OOB (Out of Bag Error) is very high when Mtry is 1, and goes down at Mtry equal 3, and reaches the bottom at Mtry equals 6, then bounces back.


#### Tuned RF model
```{r tuned model, echo = F, message = F}
rf_tuned <- randomForest(is_canceled ~ ., data = train,
                         importance = TRUE, mtry=6, ntree= 200)
print(rf_tuned)

# Prediction on test set
confusionMatrix(predict(rf_tuned,test), test$is_canceled) 

```

The accuracy of the tuned model on the test set increase 0.01, not much.

#### Variable Importance
```{r var import, echo = F, message = F}
varImpPlot(rf_tuned, main = "Variable Importance")
```


#### Partial Dependece 
```{r partial depe, echo = F, message = F}
partialPlot(rf_tuned, train, lead_time, "1", main = "Partial Dependece on Lead Time for Class 1")
partialPlot(rf_tuned, train, lead_time, "0", main = "Partial Dependece on Lead Time for Class 0")
```
To show the predictive ability of the variable, we take _LeadTime_ as an example.\
The graph above represents the the partial dependece on _LeadTime_ for Class 1 (canceled) and Class 0 (not canceled). \
If the lead time between booking date and entering/canceled date is greater than 100 days, the model tends to predict the booking will not be canceled.











